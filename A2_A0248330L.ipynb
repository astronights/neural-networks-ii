{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A0248330L.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Assignment 2\n",
        "1. This assignment is due in two weeks, at 23:59 Feb 11th 2022.\n",
        "2. There are two files to submit. Please name your .py and .ipynb file using your student number as Axxxxxx.py, Axxxxxxx.ipynb and submit it to Luminus->assignments->submissions->assignment2"
      ],
      "metadata": {
        "id": "g9iqbJnc_D1T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Policy Gradients\n",
        "\n",
        "You will implement the vanilla policy gradients algorithm, also referred to as\n",
        "REINFORCE.\n",
        "\n",
        "## Review\n",
        "\n",
        "In policy gradients, the objective is to learn a parameter $\\theta^*$ that\n",
        "maximizes the following objective:\n",
        "\n",
        "\\begin{equation}\n",
        "J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta(\\tau)}[R(\\tau)]\n",
        "\\end{equation}\n",
        "\n",
        "where $\\tau = (s_1,a_1,s_2,\\ldots,s_{T-1},a_{T-1},s_T)$ is a *trajectory*\n",
        "(also referred to as an *episode*), and factorizes as\n",
        "\n",
        "\\begin{equation}\n",
        "\\pi_\\theta(\\tau) = p(s_1)\\pi_\\theta(a_1|s_1)\\prod_{t=2}^{T} p(s_t|s_{t-1},a_{t-1})\\pi_\\theta(a_t|s_t)\n",
        "\\end{equation}\n",
        "\n",
        "and $R(\\tau)$ denotes the full trajectory reward $R(\\tau) = \\sum_{t=1}^{T}\n",
        "r(s_t,a_t)$ with $r(s_t,a_t)$ the rewards at the individual time steps.\n",
        "\n",
        "In policy gradients, we directly apply the gradient $\\nabla_\\theta$ to\n",
        "$J(\\theta)$. In order to do so, we require samples of trajectories, meaning that\n",
        "we now denote them as $\\tau_i$ for the $i$th trajectory, and have $\\tau_i =\n",
        "(s_{i1},a_{i1},s_{i2},\\ldots,s_{iT})$. When we approximate the gradient with\n",
        "samples, we get:\n",
        "\n",
        "\\begin{align*}\n",
        "\\nabla_\\theta J(\\theta) &\\approx \\frac{1}{N} \\sum_{i=1}^N \\nabla_\\theta \\log \\pi_\\theta(\\tau_i) R(\\tau_i) \\\\\n",
        "&= \\frac{1}{N}\\sum_{i=1}^N \\left( \\sum_{t=1}^T \\nabla_\\theta \\log \\pi_\\theta(a_{it}|s_{it}) \\right)  \\left( \\sum_{t=1}^{T} r(s_{it},a_{it}) \\right)\n",
        "\\end{align*}\n",
        "\n",
        "Multiplying a discount factor $\\gamma$ to the rewards can be interpreted as\n",
        "encouraging the agent to focus on rewards closer in the future, which can also\n",
        "be thought of as a means for reducing variance (because there are more\n",
        "possible futures further into the future). The discount factor can be\n",
        "incorporated in two ways, from the full trajectory:\n",
        "\n",
        "\\begin{equation}\n",
        "\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N\n",
        "\\left( \\sum_{t=1}^T \\nabla_\\theta \\log \\pi_\\theta(a_{it}|s_{it}) \\right) \n",
        "\\left( \\sum_{t=1}^T \\gamma^{t-1} r(s_{it},a_{it}) \\right)\n",
        "\\end{equation}\n",
        "\n",
        "and from the reward to go:\n",
        "\n",
        "\\begin{equation}\n",
        "\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N\n",
        "\\left( \\sum_{t=1}^T \\nabla_\\theta \\log \\pi_\\theta(a_{it}|s_{it}) \\right) \n",
        "\\left( \\sum_{t'=t}^T \\gamma^{t'-t} r(s_{it},a_{it}) \\right)\n",
        "\\end{equation}\n",
        "\n",
        "**In this assignment, we only focus on the first version: full tragectory.**\n",
        "\n"
      ],
      "metadata": {
        "id": "lCWlbWAwiOx2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Policy Gradients Implementation\n",
        "\n",
        "\n",
        "**You will need to write code in `PolicyGradient.ipynb`. The places where you need to write code are\n",
        " clearly indicated with the comments `START OF YOUR CODE` and\n",
        "`END OF YOUR CODE`. \n",
        "You do not need to change any other files for this part of the assignment.**\n",
        "\n",
        "The dataflow of the code is structured like this: \n",
        "\n",
        "- Set Up Hyperparameters and environment.\n",
        "- Build a MLP model for policy learning.\n",
        "- Initialize the agent, such as define the policy network and optimizer.\n",
        "- Forward Computation: Sample trajectories by conducting an action given an observation from the environment, and calculate sum of rewards in each trajectory, That includes `sample_action`, `sample_trajectory`, `sample_trajectories` and `sum_of_rewards`.\n",
        "- Backward Computation: Optimize the policy network based on the update rule. That contains `compute_advantage`, `estimate_return`, `get_log_prob` , `update_parameters`.\n",
        "\n",
        "## Problem 1: data sampling\n",
        "\n",
        "You need to implement any parts with a \"Problem 1\" header in the code. Here's what you need to do:\n",
        "\n",
        "- 1. Implement `sample_action`, which samples an action from $\\pi_\\theta(a|s)$. This operation will be called in `sample_trajectories`.\n",
        "- 2. Implement `sample_trajectory`, you need to call `sample_action` to obtain current action.\n",
        "- 3. Implement `sum_of_rewards`, which is the Monte Carlo estimation of the Q function. You need to estimate the q-value of each path and return a single vector for the estimated q values whose length is the sum of the lengths of the paths.\n",
        "\n",
        "## Problem 2: apply policy gradient\n",
        "You only need to implement the parts with the \"Problem 2\" header.\n",
        "\n",
        "- **Estimate return**: in `estimate_return`, normalize the advantages to have a mean of zero and a standard deviation of one.  This is a trick for reducing variance.\n",
        "- Implement `get_log_prob` to obtain $\\log \\pi_\\theta(a_{it}|s_{it})$: Given an action that the agent took in the environment, this computes the log probability of that action under $\\pi_\\theta(a|s)$. This will be used in the parameters update: \n",
        "\n",
        "\\begin{equation}\n",
        "\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N\n",
        "\\left( \\sum_{t=1}^T \\nabla_\\theta \\log \\pi_\\theta(a_{it}|s_{it}) \\right) \n",
        "\\left( \\sum_{t=1}^T \\gamma^{t-1} r(s_{it},a_{it}) \\right)\n",
        "\\end{equation}\n",
        "\n",
        "- **Update parameters**: In `update_parameters`, using the update operation `optimizer.step()` to update the parameters of the policy. You firstly need to create loss value with the inputs.\n",
        "\n"
      ],
      "metadata": {
        "id": "7UzZoFBEnaN0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Introduction: \n",
        "\n",
        "\n",
        "##[CartPole-v0](https://gym.openai.com/envs/CartPole-v0/): \n",
        "This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson in [\"Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem\"](https://ieeexplore.ieee.org/document/6313077). A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum starts upright, and the goal is to prevent it from falling over by increasing and reducing the cart's velocity.\n",
        "\n",
        "### Observation Space\n",
        "The observation is a `ndarray` with shape `(4,)` where the elements correspond to the following:\n",
        "\n",
        "| Num | Observation           | Min                  | Max                |\n",
        "|-----|-----------------------|----------------------|--------------------|\n",
        "| 0   | Cart Position         | -4.8*                |  4.8*                |\n",
        "| 1   | Cart Velocity         | -Inf                 | Inf                |\n",
        "| 2   | Pole Angle            | ~ -0.418 rad (-24°)** | ~ 0.418 rad (24°)** |\n",
        "| 3   | Pole Angular Velocity | -Inf                 | Inf                 |\n",
        "\n",
        "- `*`: the cart x-position can be observed between `(-4.8, 4.8)`, but an episode terminates if the cart leaves the\n",
        "    `(-2.4, 2.4)` range.\n",
        "- `**`: Similarly, the pole angle can be observed between  `(-.418, .418)` radians or precisely **±24°**, but an episode is\n",
        "    terminated if the pole angle is outside the `(-.2095, .2095)` range or precisely **±12°**\n",
        "\n",
        "### Action Space\n",
        "The agent take a 1-element vector for actions.\n",
        "The action space is `(action)` in `[0, 1]`, where `action` is used to push\n",
        "the cart with a fixed amount of force:\n",
        "\n",
        "| Num | Action                 |\n",
        "|-----|------------------------|\n",
        "| 0   | Push cart to the left  |\n",
        "| 1   | Push cart to the right |\n",
        "\n",
        "Note: The amount the velocity is reduced or increased is not fixed as it depends on the angle the pole is pointing.\n",
        "This is because the center of gravity of the pole increases the amount of energy needed to move the cart underneath it\n",
        "\n",
        "### Rewards\n",
        "Reward is 1 for every step taken, including the termination step.\n",
        "### Starting State\n",
        "All observations are assigned a uniform random value between (-0.05, 0.05).\n",
        "### Episode Termination\n",
        "The episode terminates of one of the following occurs:\n",
        "1. Pole Angle is more than ±12°\n",
        "2. Cart Position is more than ±2.4 (center of the cart reaches the edge of the display)\n",
        "3. Episode length is greater than 200. \n"
      ],
      "metadata": {
        "id": "vSvcalq4OcIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym==0.10.5"
      ],
      "metadata": {
        "id": "l1gjw8-xs1r8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b56462e4-d390-463d-b422-13a078811287"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gym==0.10.5\n",
            "  Downloading gym-0.10.5.tar.gz (1.5 MB)\n",
            "\u001b[?25l\r\u001b[K     |▏                               | 10 kB 17.8 MB/s eta 0:00:01\r\u001b[K     |▍                               | 20 kB 23.8 MB/s eta 0:00:01\r\u001b[K     |▋                               | 30 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |▉                               | 40 kB 31.8 MB/s eta 0:00:01\r\u001b[K     |█                               | 51 kB 32.4 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 61 kB 25.2 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 71 kB 22.2 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 81 kB 23.4 MB/s eta 0:00:01\r\u001b[K     |██                              | 92 kB 25.2 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 102 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 112 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 122 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 133 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |███                             | 143 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 153 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 163 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 174 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 184 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |████                            | 194 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 204 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 215 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 225 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |█████                           | 235 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 245 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 256 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 266 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 276 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |██████                          | 286 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 296 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 307 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 317 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 327 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |███████                         | 337 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 348 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 358 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 368 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |████████                        | 378 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 389 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 399 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 409 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 419 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 430 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 440 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 450 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 460 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 471 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 481 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 491 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 501 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 512 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 522 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 532 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 542 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 552 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 563 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 573 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 583 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 593 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 604 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 614 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 624 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 634 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 645 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 655 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 665 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 675 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 686 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 696 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 706 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 716 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 727 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 737 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 747 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 757 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 768 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 778 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 788 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 798 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 808 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 819 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 829 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 839 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 849 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 860 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 870 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 880 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 890 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 901 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 911 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 921 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 931 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 942 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 952 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 962 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 972 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 983 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 993 kB 23.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.0 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.0 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.0 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.0 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.0 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.1 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.1 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.1 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.1 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.1 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.1 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.1 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.1 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.1 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.2 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.2 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.2 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.2 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.2 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.2 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.2 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.2 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.2 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.3 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.3 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.3 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.3 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.3 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.3 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.3 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.3 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.3 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.4 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.4 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.4 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.4 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.4 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.4 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.4 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.4 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.4 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.4 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.5 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.5 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.5 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.5 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.5 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.5 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.5 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5 MB 23.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5 MB 23.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym==0.10.5) (1.19.5)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.10.5) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym==0.10.5) (1.15.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.10.5) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet>=1.2.0->gym==0.10.5) (0.16.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.5) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym==0.10.5) (1.24.3)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.10.5-py3-none-any.whl size=1581307 sha256=8ba74118a02ca3434cfbcafe0ead88892248d078e681c263df79967488ab71c6\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/2c/df/a05b548a40fae16ca400ecbeda0067e1a296499c1fbd7e0c9a\n",
            "Successfully built gym\n",
            "Installing collected packages: gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "Successfully installed gym-0.10.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qPM8ZfzReLBm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import os\n",
        "import time\n",
        "import inspect\n",
        "import sys\n",
        "from multiprocessing import Process\n",
        "import torch\n",
        "from torch import nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Up Hyperparameters"
      ],
      "metadata": {
        "id": "XMeurTX_Qnac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = 'CartPole-v0'\n",
        "# exp_name = 'vpg'\n",
        "render = False\n",
        "animate = render\n",
        "discount = 1.0\n",
        "n_iter = 101\n",
        "batch_size = 1000\n",
        "ep_len = -1.\n",
        "learning_rate = 5e-3\n",
        "reward_to_go = False\n",
        "dont_normalize_advantages = False\n",
        "seed = 1\n",
        "n_experiments = 1\n",
        "max_path_length = ep_len if ep_len > 0 else None\n",
        "min_timesteps_per_batch = batch_size\n",
        "gamma = discount\n",
        "normalize_advantages = not(dont_normalize_advantages)"
      ],
      "metadata": {
        "id": "RWAgpuL5qG5_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Up Environment"
      ],
      "metadata": {
        "id": "bjen3xsdQvaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#========================================================================================#\n",
        "# Set Up Env\n",
        "#========================================================================================#\n",
        "\n",
        "# Make the gym environment\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# Set random seeds\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "env.seed(seed)\n",
        "\n",
        "# Maximum length for episodes\n",
        "max_path_length = max_path_length or env.spec.max_episode_steps\n",
        "\n",
        "# Is this env continuous, or self.discrete? In this assignment, we only consider discrete action space.\n",
        "discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
        "\n",
        "# Observation and action sizes\n",
        "ob_dim = env.observation_space.shape[0]\n",
        "ac_dim = env.action_space.n if discrete else env.action_space.shape[0]"
      ],
      "metadata": {
        "id": "Q86wu9Q1JP_f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05d5f951-94a2-4198-a312-6f3dd6f20260"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build a MLP model for policy learning."
      ],
      "metadata": {
        "id": "mMnEZgWfSiR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, num_actions):\n",
        "        super(MLP, self).__init__()\n",
        "        self.dense1 = nn.Linear(input_size, 32)\n",
        "        self.dense2 = nn.Linear(32, 32)\n",
        "        self.dense3 = nn.Linear(32, num_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.tanh(self.dense1(x))\n",
        "        x = F.tanh(self.dense2(x))\n",
        "        out = F.softmax(self.dense3(x))\n",
        "        return out"
      ],
      "metadata": {
        "id": "FNQgI6V7erIN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Initialize Agent\n",
        "    "
      ],
      "metadata": {
        "id": "Ln23veEJLnRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy_net = MLP(input_size=ob_dim, num_actions=ac_dim)\n",
        "optimizer = torch.optim.Adam(policy_net.parameters(), lr=learning_rate)\n"
      ],
      "metadata": {
        "id": "_IKxBHwBFnDc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Sampling"
      ],
      "metadata": {
        "id": "bFVJfx6z-RgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_action(policy_parameters):\n",
        "    \"\"\"\n",
        "    Stochastically sampling from the policy distribution\n",
        "\n",
        "    arguments:\n",
        "        policy_parameters: logits of a categorical distribution over actions\n",
        "                sy_logits_na: (batch_size, self.ac_dim)\n",
        "\n",
        "    returns:\n",
        "        sy_sampled_ac: (batch_size,)\n",
        "    \"\"\"\n",
        "\n",
        "    sy_logits_na = policy_parameters\n",
        "    #========================================================================================#\n",
        "    #                           ----------PROBLEM 1----------\n",
        "    #========================================================================================#\n",
        "    # Stochastically sampling an action from the policy distribution $\\pi_\\theta(a|s)$.\n",
        "    # ------------------------------------------------------------------\n",
        "    # START OF YOUR CODE\n",
        "    # ------------------------------------------------------------------\n",
        "\n",
        "    sy_sampled_ac = torch.multinomial(sy_logits_na, 1)\n",
        "\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # END OF YOUR CODE\n",
        "    # ------------------------------------------------------------------\n",
        "\n",
        "    return sy_sampled_ac"
      ],
      "metadata": {
        "id": "ctKFEkatGfDH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_trajectory(env):\n",
        "    ob = env.reset()\n",
        "    obs, acs, rewards = [], [], []\n",
        "    steps = 0\n",
        "    while True:\n",
        "\n",
        "        obs.append(ob)\n",
        "        #====================================================================================#\n",
        "        #                           ----------PROBLEM 1----------\n",
        "        #====================================================================================#\n",
        "        # obtain the action 'ac' for current observation 'ob'\n",
        "        # ------------------------------------------------------------------\n",
        "        # START OF YOUR CODE\n",
        "        # ------------------------------------------------------------------\n",
        "        ac_probs = policy_net.forward(torch.Tensor(ob)).detach()\n",
        "        ac = sample_action(ac_probs)\n",
        "        # ------------------------------------------------------------------\n",
        "        # END OF YOUR CODE\n",
        "        # ------------------------------------------------------------------\n",
        "        ac = ac.numpy()[0]\n",
        "        acs.append(ac)\n",
        "        ob, rew, done, _ = env.step(ac)\n",
        "        rewards.append(rew)\n",
        "        steps += 1\n",
        "        if done or steps > max_path_length:\n",
        "            break\n",
        "    path = {\"observation\" : np.array(obs, dtype=np.float32),\n",
        "            \"reward\" : np.array(rewards, dtype=np.float32),\n",
        "            \"action\" : np.array(acs, dtype=np.float32)}\n",
        "    return path\n"
      ],
      "metadata": {
        "id": "Z7hSoAK0HP81"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_trajectories(itr, env):\n",
        "    \"\"\"Collect paths until we have enough timesteps, as determined by the\n",
        "    length of all paths collected in this batch.\n",
        "    \"\"\"\n",
        "    timesteps_this_batch = 0\n",
        "    paths = []\n",
        "    while True:\n",
        "        path = sample_trajectory(env)\n",
        "        paths.append(path)\n",
        "        timesteps_this_batch += len(path[\"reward\"])\n",
        "        if timesteps_this_batch > min_timesteps_per_batch:\n",
        "            break\n",
        "    return paths, timesteps_this_batch"
      ],
      "metadata": {
        "id": "UdwFJkpbHGmt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For sum of rewards, we use the total discounted reward summed over entire trajectory (regardless of which time step the Q-value should be for)."
      ],
      "metadata": {
        "id": "-c4Rgb7jC9jw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sum_of_rewards(re_n):\n",
        "    \"\"\" Monte Carlo estimation of the Q function.\n",
        "\n",
        "    let sum_of_path_lengths be the sum of the lengths of the paths sampled from\n",
        "        the function sample_trajectories\n",
        "    let num_paths be the number of paths sampled from sample_trajectories\n",
        "\n",
        "    arguments:\n",
        "        re_n: length: num_paths. Each element in re_n is a numpy array\n",
        "            containing the rewards for the particular path\n",
        "\n",
        "    returns:\n",
        "        q_n: shape: (sum_of_path_lengths). A single vector for the estimated q values\n",
        "            whose length is the sum of the lengths of the paths\n",
        "    ----------------------------------------------------------------------------------\n",
        "\n",
        "    Your code should construct numpy arrays for Q-values which will be used to compute\n",
        "    advantages.\n",
        "\n",
        "\n",
        "    You will write code for trajectory-based PG: \n",
        "\n",
        "          We use the total discounted reward summed over\n",
        "          entire trajectory (regardless of which time step the Q-value should be for).\n",
        "\n",
        "          For this case, the policy gradient estimator is\n",
        "\n",
        "              E_{tau} [sum_{t=0}^T grad log pi(a_t|s_t) * Ret(tau)]\n",
        "\n",
        "          where\n",
        "\n",
        "              tau=(s_0, a_0, ...) is a trajectory,\n",
        "              Ret(tau) = sum_{t'=0}^T gamma^t' r_{t'}.\n",
        "\n",
        "          Thus, you should compute\n",
        "\n",
        "              Q_t = Ret(tau)\n",
        "\n",
        "    Store the Q-values for all timesteps and all trajectories in a variable 'q_n',\n",
        "    like the 'ob_no' and 'ac_na' above.\n",
        "    \"\"\"\n",
        "    #====================================================================================#\n",
        "    #                           ----------PROBLEM 1----------\n",
        "    #====================================================================================#\n",
        "    # q_n: A single vector for the estimated q values whose length is the sum of the lengths of the paths.\n",
        "    # Q-values: Q_t = Ret(tau) = sum_{t'=0}^T gamma^t' r_{t'}. \n",
        "    # Store the Q-values for all timesteps and all trajectories in a variable 'q_n'.\n",
        "    # ------------------------------------------------------------------\n",
        "    # START OF YOUR CODE\n",
        "    # ------------------------------------------------------------------\n",
        "    q_n = []\n",
        "    for path in re_n:\n",
        "      temp_q_n = np.empty(len(path))\n",
        "      cur_reward = 0\n",
        "      discount_multiplier = 1\n",
        "      for val in path:\n",
        "        cur_reward = cur_reward + (discount_multiplier*val)\n",
        "        discount_multiplier = discount*discount_multiplier\n",
        "        temp_q_n.fill(cur_reward)\n",
        "      q_n.extend(temp_q_n[::-1])\n",
        "\n",
        "    # # ------------------------------------------------------------------\n",
        "    # END OF YOUR CODE\n",
        "    # ------------------------------------------------------------------\n",
        "    return q_n"
      ],
      "metadata": {
        "id": "7HFdZ45SHm-g"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply Policy Gradient\n",
        "\n",
        "We firstly need to estimate return `estimate_return` and calculate log probability of actions `get_log_prob`. Then we can update parameters based on the rule:\n",
        "\n",
        "\\begin{equation}\n",
        "\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N\n",
        "\\left( \\sum_{t=1}^T \\nabla_\\theta \\log \\pi_\\theta(a_{it}|s_{it}) \\right) \n",
        "\\left( \\sum_{t=1}^T \\gamma^{t-1} r(s_{it},a_{it}) \\right)\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "W86DkWbVgNvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_advantage(ob_no, q_n):\n",
        "  \n",
        "    adv_n = q_n.copy()\n",
        "    return adv_n"
      ],
      "metadata": {
        "id": "3gdKRz2UH6BE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_return(ob_no, re_n):\n",
        "    \"\"\" Estimates the returns over a set of trajectories.\n",
        "\n",
        "    let sum_of_path_lengths be the sum of the lengths of the paths sampled from\n",
        "        sample_trajectories\n",
        "    let num_paths be the number of paths sampled from sample_trajectories\n",
        "\n",
        "    arguments:\n",
        "        ob_no: shape: (sum_of_path_lengths, ob_dim)\n",
        "        re_n: length: num_paths. Each element in re_n is a numpy array\n",
        "            containing the rewards for the particular path\n",
        "\n",
        "    returns:\n",
        "        q_n: shape: (sum_of_path_lengths). A single vector for the estimated q values\n",
        "            whose length is the sum of the lengths of the paths\n",
        "        adv_n: shape: (sum_of_path_lengths). A single vector for the estimated\n",
        "            advantages whose length is the sum of the lengths of the paths\n",
        "    \"\"\"\n",
        "    q_n = sum_of_rewards(re_n)\n",
        "    adv_n = compute_advantage(ob_no, q_n)\n",
        "    #====================================================================================#\n",
        "    #                           ----------PROBLEM 2----------\n",
        "    # Advantage Normalization\n",
        "    #====================================================================================#\n",
        "    if normalize_advantages:\n",
        "        # On the next line, implement a trick which is known empirically to reduce variance\n",
        "        # in policy gradient methods: normalize adv_n to have mean zero and std=1.\n",
        "        # ------------------------------------------------------------------\n",
        "        # START OF YOUR CODE\n",
        "        # ------------------------------------------------------------------\n",
        "\n",
        "        adv_n = (adv_n - np.mean(adv_n)) / (np.std(adv_n) + 1e-8)\n",
        "\n",
        "        # ------------------------------------------------------------------\n",
        "        # END OF YOUR CODE\n",
        "        # ------------------------------------------------------------------\n",
        "    return q_n, adv_n"
      ],
      "metadata": {
        "id": "p0bSKK4KICGo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_log_prob(policy_parameters, sy_ac_na):\n",
        "    \"\"\"\n",
        "    Computing the log probability of a set of actions that were actually taken according to the policy\n",
        "\n",
        "    arguments:\n",
        "        policy_parameters: logits of a categorical distribution over actions\n",
        "                sy_logits_na: (batch_size, self.ac_dim)\n",
        "\n",
        "        sy_ac_na: (batch_size,)\n",
        "\n",
        "    returns:\n",
        "        sy_logprob_n: (batch_size)\n",
        "\n",
        "    Hint:\n",
        "        For the discrete case, use the log probability under a categorical distribution.\n",
        "    \"\"\"\n",
        "\n",
        "    sy_logits_na = policy_parameters\n",
        "    #========================================================================================#\n",
        "    #                           ----------PROBLEM 2----------\n",
        "    #========================================================================================#\n",
        "    # sy_logprob_n = \\sum_{t=1}^T \\log \\pi_\\theta(a_{it}|s_{it})\n",
        "    # ------------------------------------------------------------------\n",
        "    # START OF YOUR CODE\n",
        "    # ------------------------------------------------------------------\n",
        "    sy_logprob_n = torch.distributions.Categorical(sy_logits_na).log_prob(torch.Tensor(sy_ac_na))\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # END OF YOUR CODE\n",
        "    # ------------------------------------------------------------------\n",
        "    return sy_logprob_n"
      ],
      "metadata": {
        "id": "mfS5P1B6Gq5B"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(ob_no, ac_na, q_n, adv_n):\n",
        "    \"\"\"\n",
        "    Update the parameters of the policy and (possibly) the neural network baseline,\n",
        "    which is trained to approximate the value function.\n",
        "\n",
        "    arguments:\n",
        "        ob_no: shape: (sum_of_path_lengths, ob_dim)\n",
        "        ac_na: shape: (sum_of_path_lengths).\n",
        "        q_n: shape: (sum_of_path_lengths). A single vector for the estimated q values\n",
        "            whose length is the sum of the lengths of the paths\n",
        "        adv_n: shape: (sum_of_path_lengths). A single vector for the estimated\n",
        "            advantages whose length is the sum of the lengths of the paths\n",
        "\n",
        "    returns:\n",
        "        nothing\n",
        "    \"\"\"\n",
        "    #====================================================================================#\n",
        "    #                           ----------PROBLEM 2----------\n",
        "    #====================================================================================#\n",
        "    # Performing the Policy Update based on the current batch of rollouts.\n",
        "    # \n",
        "    # ------------------------------------------------------------------\n",
        "    # START OF YOUR CODE\n",
        "    # ------------------------------------------------------------------\n",
        "    policy_params = policy_net.forward(torch.Tensor(ob_no))\n",
        "    log_probs = get_log_prob(policy_params, ac_na)\n",
        "    J = - torch.mean(log_probs * torch.Tensor(adv_n))\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    J.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # END OF YOUR CODE\n",
        "    # ------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "S_xQL3kPINdl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop."
      ],
      "metadata": {
        "id": "4zp8VyVQgQ-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Running experiment with seed %d'%seed)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "total_timesteps = 0\n",
        "\n",
        "return_data = []\n",
        "\n",
        "for itr in range(n_iter):\n",
        "\n",
        "    paths, timesteps_this_batch = sample_trajectories(itr, env)\n",
        "    total_timesteps += timesteps_this_batch\n",
        "\n",
        "    # Build arrays for observation, action for the policy gradient update by\n",
        "    # concatenating across paths\n",
        "    ob_no = np.concatenate([path[\"observation\"] for path in paths])\n",
        "    ac_na = np.concatenate([path[\"action\"] for path in paths])\n",
        "\n",
        "    re_n = [path[\"reward\"] for path in paths]\n",
        "\n",
        "    q_n, adv_n = estimate_return(ob_no, re_n)\n",
        "\n",
        "\n",
        "    update_parameters(ob_no, ac_na, q_n, adv_n)\n",
        "\n",
        "    # Log diagnostics\n",
        "    returns = [path[\"reward\"].sum() for path in paths]\n",
        "\n",
        "    if itr%10 == 0:\n",
        "        print(\"********** Iteration %i ************\"%itr)\n",
        "        ep_lengths = [len(path[\"reward\"]) for path in paths]\n",
        "        print(\"Time: \", time.time() - start)\n",
        "        print(\"Iteration: \", itr)\n",
        "        print(\"AverageReturn: \", np.mean(returns))\n",
        "        print(\"StdReturn: \", np.std(returns))\n",
        "        print(\"MaxReturn: \", np.max(returns))\n",
        "        print(\"MinReturn\", np.min(returns))\n",
        "        print(\"EpLenMean: \", np.mean(ep_lengths))\n",
        "        print(\"EpLenStd: \", np.std(ep_lengths))\n",
        "        print(\"TimestepsThisBatch: \", timesteps_this_batch)\n",
        "        print(\"TimestepsSoFar: \", total_timesteps)\n",
        "    return_data.append(np.mean(returns))"
      ],
      "metadata": {
        "id": "NqS80le8pjC8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2352e832-c4bd-4051-8e6c-d3479c50196c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running experiment with seed 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  if sys.path[0] == '':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********** Iteration 0 ************\n",
            "Time:  0.5708930492401123\n",
            "Iteration:  0\n",
            "AverageReturn:  26.410257\n",
            "StdReturn:  11.369591\n",
            "MaxReturn:  54.0\n",
            "MinReturn 9.0\n",
            "EpLenMean:  26.41025641025641\n",
            "EpLenStd:  11.369590454969824\n",
            "TimestepsThisBatch:  1030\n",
            "TimestepsSoFar:  1030\n",
            "********** Iteration 10 ************\n",
            "Time:  2.9149317741394043\n",
            "Iteration:  10\n",
            "AverageReturn:  56.666668\n",
            "StdReturn:  24.016197\n",
            "MaxReturn:  113.0\n",
            "MinReturn 21.0\n",
            "EpLenMean:  56.666666666666664\n",
            "EpLenStd:  24.0161982373934\n",
            "TimestepsThisBatch:  1020\n",
            "TimestepsSoFar:  11382\n",
            "********** Iteration 20 ************\n",
            "Time:  5.91389799118042\n",
            "Iteration:  20\n",
            "AverageReturn:  126.5\n",
            "StdReturn:  40.996952\n",
            "MaxReturn:  187.0\n",
            "MinReturn 70.0\n",
            "EpLenMean:  126.5\n",
            "EpLenStd:  40.996951106149346\n",
            "TimestepsThisBatch:  1012\n",
            "TimestepsSoFar:  21635\n",
            "********** Iteration 30 ************\n",
            "Time:  11.280832529067993\n",
            "Iteration:  30\n",
            "AverageReturn:  195.66667\n",
            "StdReturn:  9.689628\n",
            "MaxReturn:  200.0\n",
            "MinReturn 174.0\n",
            "EpLenMean:  195.66666666666666\n",
            "EpLenStd:  9.689627902499089\n",
            "TimestepsThisBatch:  1174\n",
            "TimestepsSoFar:  32775\n",
            "********** Iteration 40 ************\n",
            "Time:  16.04241681098938\n",
            "Iteration:  40\n",
            "AverageReturn:  200.0\n",
            "StdReturn:  0.0\n",
            "MaxReturn:  200.0\n",
            "MinReturn 200.0\n",
            "EpLenMean:  200.0\n",
            "EpLenStd:  0.0\n",
            "TimestepsThisBatch:  1200\n",
            "TimestepsSoFar:  44305\n",
            "********** Iteration 50 ************\n",
            "Time:  18.468026161193848\n",
            "Iteration:  50\n",
            "AverageReturn:  200.0\n",
            "StdReturn:  0.0\n",
            "MaxReturn:  200.0\n",
            "MinReturn 200.0\n",
            "EpLenMean:  200.0\n",
            "EpLenStd:  0.0\n",
            "TimestepsThisBatch:  1200\n",
            "TimestepsSoFar:  56304\n",
            "********** Iteration 60 ************\n",
            "Time:  20.88632845878601\n",
            "Iteration:  60\n",
            "AverageReturn:  200.0\n",
            "StdReturn:  0.0\n",
            "MaxReturn:  200.0\n",
            "MinReturn 200.0\n",
            "EpLenMean:  200.0\n",
            "EpLenStd:  0.0\n",
            "TimestepsThisBatch:  1200\n",
            "TimestepsSoFar:  68298\n",
            "********** Iteration 70 ************\n",
            "Time:  23.329230070114136\n",
            "Iteration:  70\n",
            "AverageReturn:  200.0\n",
            "StdReturn:  0.0\n",
            "MaxReturn:  200.0\n",
            "MinReturn 200.0\n",
            "EpLenMean:  200.0\n",
            "EpLenStd:  0.0\n",
            "TimestepsThisBatch:  1200\n",
            "TimestepsSoFar:  80298\n",
            "********** Iteration 80 ************\n",
            "Time:  25.76165795326233\n",
            "Iteration:  80\n",
            "AverageReturn:  200.0\n",
            "StdReturn:  0.0\n",
            "MaxReturn:  200.0\n",
            "MinReturn 200.0\n",
            "EpLenMean:  200.0\n",
            "EpLenStd:  0.0\n",
            "TimestepsThisBatch:  1200\n",
            "TimestepsSoFar:  92298\n",
            "********** Iteration 90 ************\n",
            "Time:  28.23506474494934\n",
            "Iteration:  90\n",
            "AverageReturn:  200.0\n",
            "StdReturn:  0.0\n",
            "MaxReturn:  200.0\n",
            "MinReturn 200.0\n",
            "EpLenMean:  200.0\n",
            "EpLenStd:  0.0\n",
            "TimestepsThisBatch:  1200\n",
            "TimestepsSoFar:  104298\n",
            "********** Iteration 100 ************\n",
            "Time:  30.64638876914978\n",
            "Iteration:  100\n",
            "AverageReturn:  200.0\n",
            "StdReturn:  0.0\n",
            "MaxReturn:  200.0\n",
            "MinReturn 200.0\n",
            "EpLenMean:  200.0\n",
            "EpLenStd:  0.0\n",
            "TimestepsThisBatch:  1200\n",
            "TimestepsSoFar:  116298\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot Average-Return curve.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "D4ifa06fgs7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(return_data)\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Average Return\")"
      ],
      "metadata": {
        "id": "UvnRU2vT0xIj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "c4abfa6f-66d7-4dea-8a78-7274abd93bc6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Average Return')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}